{
    "server_name": "potato annotator",

    "annotation_task_name": "Relevance and Stance",

    # Potato will write the annotation file for all annotations to this
    # directory, as well as per-annotator output files and state information
    # necessary to restart annotation.
    "output_annotation_dir": "annotation_output/<dataset_to_annotate>", # CHECK: set this to match the 'data_files' name

    # The output format for the all-annotator data. Allowed formats are:
    # * jsonl
    # * json (same output as jsonl)
    # * csv
    # * tsv
    #
    "output_annotation_format": "tsv",

    # If annotators are using a codebook, this will be linked at the top to the
    # instance for easy access
    "annotation_codebook_url": "",

    "data_files": [
       "<dataset_to_annotate>", # CHECK: set this to the desired file for the study
    ],

    "item_properties": {
        "id_key": "id",
        "text_key": "text", # when preparing the data, make sure there's one field text with all info, see the example pilot_data.tsv
    },

  #list_as_text is used when the input text is actually a list of texts, usually used for best-worst-scaling
    "list_as_text": {
      "text_list_prefix_type": 'None',
      "horizontal": True,
    },

    "user_config": {
      "allow_all_users": True,
      "users": [  ],
    },

    "prolific": { # CHECK: comment this out for local test runs
        "config_file_path": 'configs/prolific_config.yaml'
    },

    # #defining the ways annotators entering the annotation system
    "login": {
       "type": 'url_direct',    #can be 'password' or 'url_direct'
       "url_argument": ['PROLIFIC_PID', 'STUDY_ID', 'SESSION_ID'] # prolific PID is the annotator id, study ID is the prolific id of our study and we can also record prolific session id (the username on potato will be the concatenation of these)
    },

    #the jumping-to-id function will be disabled if "jumping_to_id_disabled" is True
    "jumping_to_id_disabled": False,

  #the navigation bar will be hidden to the annotators if "hide_navbar" is True
    "hide_navbar": True,

  # define the surveyflow of the system, set up the pages before and after the data annotation page
    "surveyflow": {
      "on": True,
      "order" : ['pre_annotation', 'post_annotation'],
      "pre_annotation": ['surveyflow/intro.jsonl', 'surveyflow/instruction.jsonl', 'surveyflow/consent.jsonl', 'surveyflow/pep_talk.jsonl'],
      "post_annotation": ['surveyflow/experience.jsonl','surveyflow/end.jsonl'], # CHECK: configure end with prolific end code
      # If set, we will automatically generate testing questions similar to the annotation instances, but explicitly ask the annotator to choose one option
      "testing": ['surveyflow/testing.jsonl'], # not same as prestudy - this interleaves testing questions during the annotations, e.g. attention tests.
    },

    "automatic_assignment": {
      #whether do automatic task assignment for annotators, default False.
      "on": True,
      "output_filename": 'task_assignment.json',
      "sampling_strategy": 'random',
      "labels_per_instance": 1,
      "instance_per_annotator": 32, # CHECK: tune this value - 32
      "test_question_per_annotator": 2,
    },


    # How many seconds do you want the annotators spend on each instance, after
    # that, an alert will be sent per alert_time_each_instance seconds.
    "alert_time_each_instance": 1800, # 30 minutes time limit (really annoying, also applies to instructions pages)
    "horizontal_key_bindings": true,
    "annotation_schemes": [      
      {
          "annotation_type": "radio",
          "name": "Is the evidence relevant?",
          "description": "Is the evidence relevant? Does the evidence contain any information that 1) directly supports or refutes the claim, 2) is topically related to the topic or entities of the claim or claimant (same people, places, organisations, etc.), or 3) can be seen as implicitly referring to the claim?",
          "labels": [
            {
              "name": "True",
              "tooltip": "Assign a value of True for this property if the evidence discusses the same subject as the claim, regardless of whether it fully supports or refutes the claim. Even if the evidence is insufficient to determine the claimâ€™s veracity on its own, it should still be considered relevant if it addresses the topic of the claim or discusses the same entities. Also, assign a value of True if the evidence could be seen as implicitly referring to the claim, even if it is not explicitly topically related.",
              "key_value": '1'
            },
            {
              "name": "False",
              "tooltip": "Assign a value of False if the evidence is entirely unrelated to the claim or its subject matter. ",
              "key_value": '2'
            }
          ],                      
      },       

      {
          "annotation_type": "radio",
          "name": "What is the stance of the evidence?",
          "description": "What is the stance of the evidence? Each provided evidence should correspond to one of the stances listed below. Evidence marked as relevant=False should be annotated as 'not_applicable'.",
          "label_requirement": {"required":True}, # the prestudy test will crash if no stance is set                      
          "labels": [
            {
              "name": "supports",
              "tooltip": "The evidence contains sufficient information that supports the given claim. I.e. the claim can be considered true based on the information in the corresponding evidence. Repetitions or quotes of the claim in the evidence do not count as support if it is clear that they are there simply to show that the claim is being addressed. For claims consisting of several parts, all parts need to be supported by the evidence to mark it as supporting.",
              "key_value": '3'
            },
            {
              "name": "insufficient-supports",
              "tooltip": "The evidence contains relevant information but is not sufficient for verifying the claim. Nevertheless, it is leaning towards supporting the claim.",
              "key_value": '4'
            },
            {
              "name": "insufficient-neutral",
              "tooltip": "The evidence contains relevant information but is not sufficient for verifying the claim. It does not lean towards supporting or refuting the claim.",
              "key_value": '5'
            },
            {
              "name": "insufficient-contradictory",
              "tooltip": "The evidence contains relevant information but is contradictory and therefore not sufficient for verifying the claim. For example, some part of the evidence supports the claim while another part refutes it.",
              "key_value": '6'
            },
            {
              "name": "insufficient-refutes",
              "tooltip": "The evidence contains relevant information but is not sufficient for verifying the claim. Nevertheless, it is leaning towards refuting the claim.",
              "key_value": '7'
            },
            {
              "name": "refutes",
              "tooltip": "The evidence contains sufficient information that refutes the given claim. I.e. the claim can be considered false based on the information in the corresponding evidence. For claims consisting of several parts, it is enough if only one part of the claim is refuted by the evidence to mark it as refuting.",
              "key_value": '8'
            },
            {
              "name": "not_applicable",
              "tooltip": "In case of annotated relevant=False in the previous task, mark the stance as 'not_applicable'.",
              "key_value": '9'
            },
          ],                      
      },

      {
        "annotation_type": "text",
        "name": "Was there a quality issue?",
        "description": "Was there a quality issue with this sample that prevented you from annotating it as instructed? If so, shortly describe the issue here. Leave this box empty if there was no issue.",
      }       

  ],

    # The html that changes the visualiztation for your task. Change this file
    # to influence the layout and description of your task. This is not a full
    # HTML page, just the piece that does lays out your task's pieces
    #"html_layout": "templates/examples/fixed_keybinding_layout.html",
    "html_layout": "templates/layout.html",
    "surveyflow_html_layout": "templates/layout-survey.html",

    # The core UI files for Potato. You should not need to change these normally.
    #
    # Exceptions to this might include:
    # 1) You want to add custom CSS/fonts to style your task
    # 2) Your layout requires additional JS/assets to render
    # 3) You want to support additional keybinding magic
    #
    # if you want to use your own template,
    # please replace the string as a path to the template
    "base_html_template": "default",
    "header_file": "default",

    # This is where the actual HTML files will be generated
    "site_dir": "default"

}
